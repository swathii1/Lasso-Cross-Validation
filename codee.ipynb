{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34bc16a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC values are better for the AIC model with the value of alpha\n",
      "AIC and BIC hold the same interpretation in terms of model comparison.\n",
      "That is, the larger difference in either AIC or BIC indicates stronger evidence for one model over the other \n",
      "(the lower the better). It's just the the AIC doesn't penalize the number of parameters as strongly as BIC.\n",
      " There is also a correction to the AIC (the AICc) that is used for smaller sample sizes. \n",
      "BIC is greater than AIC in these cases.However that need not necessarily be the case.\n",
      "When the number of observations is large the Akaike Information Criterion (AIC) and the small-sample corrected Akaike Information Criterion (AICc)\n",
      "become extremely similar because AICc converges to AIC\n",
      "Therefore we gain (or lose) almost nothing by switching between the two criteria.\n",
      "AIC is lower than AICc in all of the models\n",
      "\n",
      "######LASSO MODEL######\n",
      "Train_size:  0.8\n",
      "R2:  0.5742378400198931\n",
      "alpha:  1\n",
      "AIC:  481.5073586106532\n",
      "BIC:  498.18154505337037\n",
      "AICC 483.06291416620877\n",
      "\n",
      "\n",
      "Train_size:  0.2\n",
      "R2:  0.5108320783990352\n",
      "alpha:  1\n",
      "AIC:  1991.4297804959097\n",
      "BIC:  2017.7861182154036\n",
      "AICC 1991.789909113273\n",
      "\n",
      "\n",
      "R2 value is 57% that is 57 %of the variation can be explained using this model\n",
      "\n",
      "######AIC MODEL######\n",
      "Train_size:  0.8\n",
      "R2:  0.8276757484417713\n",
      "alpha:  0.04119086503016513\n",
      "AIC:  409.1471190196736\n",
      "BIC:  425.82130546239074\n",
      "AICC 410.70267457522914\n",
      "\n",
      "Lasso with AIC alpha\n",
      "Train_size:  0.8\n",
      "R2:  0.8276766393399528\n",
      "alpha:  0.04119086503016513\n",
      "AIC:  409.14670542705323\n",
      "BIC:  425.8208918697704\n",
      "AICC 410.7022609826088\n",
      "\n",
      "\n",
      "Train_size:  0.2\n",
      "R2:  0.8151021434699341\n",
      "alpha:  0.05329386295562331\n",
      "AIC:  1681.0739522990998\n",
      "BIC:  1707.4302900185937\n",
      "AICC 1681.4340809164632\n",
      "\n",
      "Lasso with AIC alpha\n",
      "Train_size:  0.2\n",
      "R2:  0.8151020857786639\n",
      "alpha:  0.05329386295562331\n",
      "AIC:  1681.0740518324997\n",
      "BIC:  1707.4303895519936\n",
      "AICC 1681.434180449863\n",
      "\n",
      "\n",
      "\n",
      "######BIC MODEL######\n",
      "Train_size:  0.8\n",
      "R2:  0.8276757484417713\n",
      "alpha:  0.04119086503016513\n",
      "AIC:  409.1471190196736\n",
      "BIC:  425.82130546239074\n",
      "AICC 410.70267457522914\n",
      "\n",
      "Lasso with BIC alpha\n",
      "Train_size:  0.8\n",
      "R2:  0.8276766393399528\n",
      "alpha:  0.04119086503016513\n",
      "AIC:  409.14670542705323\n",
      "BIC:  425.8208918697704\n",
      "AICC 410.7022609826088\n",
      "\n",
      "\n",
      "Train_size:  0.2\n",
      "R2:  0.8151021434699341\n",
      "alpha:  0.05329386295562331\n",
      "AIC:  1681.0739522990998\n",
      "BIC:  1707.4302900185937\n",
      "AICC 1681.4340809164632\n",
      "\n",
      "Lasso with BIC alpha\n",
      "Train_size:  0.2\n",
      "R2:  0.8151020857786639\n",
      "alpha:  0.05329386295562331\n",
      "AIC:  1681.0740518324997\n",
      "BIC:  1707.4303895519936\n",
      "AICC 1681.434180449863\n",
      "\n",
      "\n",
      "AIC values indicate a better-fit model, and a model with a delta-AIC (the difference between the two AIC\n",
      "values being compared) of more than -2 is considered significantly better than the model in comparison\n",
      "The selected alpha corresponds to the minimum of the AIC or BIC criterion\n",
      "It fits by computing the criterion on the specified in-sample collection. Both criteria use the training set error to\n",
      "predict the model generalization error and penalize this too optimistic error.\n",
      "This cost, however, is contingent on accurate measurement of the degrees of freedom and noise variance.\n",
      "Both are calculated for large samples (asymptotic results) and assume that the model is true, that is, that the data are\n",
      "created by the model.\n",
      "\n",
      "######10 fold CV######\n",
      "The R2 scores are \n",
      " [0.8347733  0.78025127 0.82945371 0.81925475 0.86569983 0.80436448\n",
      " 0.75465228 0.81174716 0.83021697 0.7390123 ]\n",
      "Predictions: \n",
      " [14.87399219 14.24751504 15.31064154 15.31137832 14.87626431 10.73831183\n",
      " 10.25788709 10.66538449  9.72102175 13.0308881  15.16064917 14.08791568\n",
      " 14.12467203 20.40701352 24.65883579 18.87296522 19.36154069 20.726466\n",
      " 25.64088318 27.04041301 20.81264953 22.34039841 22.63864582 23.1023691\n",
      " 20.48719808  7.72072937  8.43231423  8.35266873  6.21517002 26.73791667\n",
      " 23.17543903 26.27927171 24.403502   21.74492559 16.0085977  17.49576494\n",
      " 17.82259878 17.32497594 11.62386434 10.66821778 12.11518543 11.8611434\n",
      "  6.78286365  8.85898169  5.91172702 20.14533137 22.81169039 17.80330792\n",
      " 18.67133718 23.59656909 25.11423008 25.57642619 25.19390482 28.96252333\n",
      " 29.40998434 27.7781859  25.04868778 26.25149623 24.32097195 26.53445693\n",
      " 23.84890219 24.34287648 11.73921778 11.75252872 12.33972601 13.06842658\n",
      " 15.02670337  9.78322403 10.44066992 10.88180833 11.42053912 26.26078183\n",
      " 13.59080909 12.94636115 11.2781237  12.84130557 20.58251462 23.90111674\n",
      " 20.73288226 25.66774577 23.24395143 25.95507592 25.01278333 24.35067073\n",
      " 27.62504202 13.43096174 15.81589145 14.70193863 13.75517289 15.55858323\n",
      "  8.65767827 12.18538288 12.22009691 12.57600898 10.48875998  8.99984408\n",
      " 15.54479493 19.79641947 19.34010743 21.43123983 21.48905862 20.85577133\n",
      " 28.45036559  8.99411789  9.16404605 10.29368138 10.75139931 22.46201778\n",
      " 27.50668229 24.79587643 26.60489691 28.32930823 24.8347061  22.62919208\n",
      " 25.6088315  14.12214745 11.75604442 28.59942918 26.62712289 23.69144354\n",
      " 21.90161288 18.05621012 23.00700389 23.36304842 16.52630506 20.36095454\n",
      " 21.62994319 21.86869599 19.92193312 30.0654655  24.29396559 30.60079687\n",
      " 24.2210118  16.77448704 17.81138555 17.27570685 13.72172969 10.85881364\n",
      " 11.5352374  10.64390713 12.76542949 26.81585643 28.29573774 26.05508161\n",
      " 32.07579019 30.1023984  25.76614856 27.40279702 26.74089677 26.26521587\n",
      " 27.19259946 28.00605878 20.5860274  19.59638877 21.0848103  22.63055789\n",
      " 12.24119458 13.51930595 12.30285644 11.60720565 16.66532343 16.69361535\n",
      " 18.27246385 17.52911907 21.7400547  20.43679813 21.27084126 29.25290743\n",
      " 24.15895309 22.81565783 24.38717666 26.12305866 27.51099514 27.3242259\n",
      " 21.06128525 29.06288175 20.96235245 24.80651264 23.05534938 22.77848048\n",
      " 24.32869813 32.30809672 26.51927604 28.57683271 25.19778159 26.79232205\n",
      " 28.34564291 14.52827642 15.0210167  16.81676499 15.5127646  21.27815125\n",
      " 20.78676609 22.79541206 22.61474805 29.10675923 28.37120875 29.98496141\n",
      " 32.66649336 18.76732362 20.35437542 18.93021222 22.83752869 30.66044663\n",
      " 31.33874499 30.13154796 25.29512351 22.41653499 16.7493889  22.44403841\n",
      " 25.13206397 16.95845479 13.71575527 16.37538654 17.34339389 18.04839518\n",
      " 31.79577435 27.98515977 31.74960437 27.48400567 32.13853449 17.55263185\n",
      " 16.741755   16.17607841 15.20030879 20.74966516 21.13075583 19.91552673\n",
      " 21.19076057 16.82036092 16.16129262 16.09365409 15.71328171 30.779112\n",
      " 25.14667963 30.64689995 24.72236826 29.10174698 28.36177772 32.45162841\n",
      " 28.94030289 26.34087729 25.97628586 26.75339917 31.91057675 31.34918446\n",
      " 33.16357982 32.00237177 34.02708169 21.97214823 19.96036315 20.51145886\n",
      " 21.30080806 23.39255168 24.29262604 25.78686508 21.83633402 23.68223471\n",
      " 22.10543419 23.73084165 20.67173793 22.13081329 21.73460057 20.8048132\n",
      " 23.04534607 17.42937105 29.08297573 29.08433339 30.63905903 28.17267191\n",
      " 29.7285075  25.71384205 25.25179909 30.37670468 25.69679411 23.71297838\n",
      " 25.98798094 21.96813558 31.07718811 31.91872691 23.59713698 25.54345121\n",
      " 25.71046854 24.23929886 22.88488802 20.03157247 20.52479833 19.28277914\n",
      " 19.48820812 16.98840349 19.04333827 21.11500767 19.64062616 32.21285871\n",
      " 33.58616551 31.02707147 26.2602938  23.20176865 20.5535459  26.12936557\n",
      " 23.5355475  29.17561522 29.70364491 33.75504658 30.89722393 27.09239237\n",
      " 26.36859817 25.73870056 27.48715554 31.55065337 34.82325995 30.69026594\n",
      " 34.31381976 27.82956078 26.56329908 26.0744917  23.6133287  31.45780514\n",
      " 30.22638254 31.48641547 31.33528164 32.89221677 33.55163975 26.52629364\n",
      " 33.42935719 32.14112447 30.54367801 26.72954695 25.88522595 34.86364092\n",
      " 32.90199479 33.84144921 33.98319578 27.20115406 30.97964967 29.76209398\n",
      " 25.7521145  32.47611395 29.35806784 28.76617558 28.62518498 27.36637239\n",
      " 29.81462542 36.44907898 32.75439402 36.73520181 34.67192259 35.53795519\n",
      " 34.75416951 35.05325412 30.67364642 32.10998631 30.18504214 32.18779019\n",
      " 31.3682336  33.93807966 33.04619235 31.1447284  31.74195414 26.83932359\n",
      " 26.52853105 29.25865847 27.91129209 24.07576326 23.21177423 26.41974614\n",
      " 24.14010536 29.34885373 29.10903801 30.36074851 29.17160433 29.70217617\n",
      " 29.16189659 27.88944242 27.37509191 34.65589805 35.8889151  36.37425246\n",
      " 32.39581085 32.16920813 34.80054831 34.77173262 34.51552113 35.59744793\n",
      " 36.20346424 35.84946123 27.35713401 27.54109562 29.78396179 28.8332813\n",
      " 32.03879039 30.71320765 27.57298295 28.5518067  34.37877056 31.0987835\n",
      " 29.09494101 28.78623402]\n",
      "The R2 values are better than in the rest of the models.Each of it explains the variation in the model\n",
      "alpha: one technique uses simply the training set and some information criterion to identify the optimal value of alpha, \n",
      "while another uses cross-validation.\n",
      "In this case, both ways are equally effective. Even in terms of computational performance, the in-sample hyperparameter\n",
      "selection seems to be effective. It can only be employed when the number of samples compared to the number of \n",
      "characteristics is large enough.\n",
      "That's why cross-validation hyperparameter optimization is a safe strategy: it works in a variety of situations.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LassoLarsIC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "def calculateAic(y, y_pred, n, k):\n",
    "    res = y - y_pred\n",
    "    logL = -(n * 1/2) * (1 + np.log(2 * np.pi)) - (n / 2) * np.log(res.dot(res) / n)\n",
    "\n",
    "    AIC = (-2 * logL) + (2 * k)\n",
    "    # print(\"\\nThe AIC is \", AIC)\n",
    "    return AIC\n",
    "    \n",
    "def calculateBic(y, y_pred,n, k):\n",
    "    res = y - y_pred\n",
    "    logL = -(n/2) * (1 + np.log(2 * np.pi)) - (n / 2) * np.log(res.dot(res) / n)\n",
    "\n",
    "    BIC = (-2 * logL) + (k * np.log(n))\n",
    "    # print(\"\\nThe BIC is \", BIC)\n",
    "    return BIC\n",
    "    \n",
    "def calculateAicc(inputAic, n , k):\n",
    "    aicc = inputAic + 2*k*(k+1)/(n - k -1)\n",
    "    return aicc\n",
    "\n",
    "def createTrainTestData(dframe, train_size, seed, Y_col_name):\n",
    "    training_data, testing_data = train_test_split (dframe, \n",
    "                                                    train_size=train_size, \n",
    "                                                    random_state=random_seed)\n",
    "    \n",
    "\n",
    "    X_train = training_data.drop(Y_col_name ,axis = 1).to_numpy()\n",
    "    \n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "\n",
    "    Y_train = training_data[Y_col_name].to_numpy()\n",
    "    X_test = testing_data.drop(Y_col_name, axis = 1).to_numpy()\n",
    "    X_test = scaler.transform(X_test)\n",
    "    Y_test = testing_data[Y_col_name].to_numpy()\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def runCriterionModel(criteria, seed, train_size, data, Y_col_name):\n",
    "    \n",
    "    X_train, Y_train, X_test, Y_test = createTrainTestData(data, \n",
    "                                                           train_size, \n",
    "                                                           seed, Y_col_name)\n",
    "    \n",
    "    \n",
    "    model = LassoLarsIC(criterion=criteria, normalize=False)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    score_model = model.score(X_test, Y_test)\n",
    "    aic = calculateAic(Y_test, Y_pred, np.shape(X_test)[0], np.shape(X_test)[1])\n",
    "    bic = calculateBic(Y_test, Y_pred, np.shape(X_test)[0], np.shape(X_test)[1])\n",
    "    # print(\"n : \", np.shape(X_test)[0])\n",
    "    # print(\"k : \", np.shape(X_test)[1])\n",
    "    aicc = calculateAicc(aic, np.shape(X_test)[0], np.shape(X_test)[1])\n",
    "    \n",
    "    printData(train_size, score_model, model.alpha_, aic, bic, aicc)\n",
    "    \n",
    "    return model.alpha_\n",
    "    \n",
    "def runLassoModel(seed, train_size, data, Y_col_name, inputAlpha):\n",
    "    \n",
    "    X_train, Y_train, X_test, Y_test = createTrainTestData(data, \n",
    "                                                           train_size, \n",
    "                                                           seed, Y_col_name)\n",
    "    \n",
    "    mdl_lasso = Lasso(alpha = inputAlpha)\n",
    "    mdl_lasso.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = mdl_lasso.predict(X_test)\n",
    "\n",
    "    score_model = mdl_lasso.score(X_test, Y_test)\n",
    "    aic = calculateAic(Y_test, Y_pred, np.shape(X_test)[0], np.shape(X_test)[1])\n",
    "    bic = calculateBic(Y_test, Y_pred, np.shape(X_test)[0], np.shape(X_test)[1])\n",
    "    # print(\"n : \", np.shape(X_test)[0])\n",
    "    # print(\"k : \", np.shape(X_test)[1])\n",
    "    aicc = calculateAicc(aic, np.shape(X_test)[0], np.shape(X_test)[1])\n",
    "    \n",
    "    printData(train_size, score_model, inputAlpha, aic, bic, aicc)\n",
    "    \n",
    "#4 Calculate AIC, AICc (i.e. Corrected AIC) and BIC for the models you built in question 1 and question 2.\n",
    "def printData(train_size, Rscore, alpha, aic, bic, aicc):\n",
    "    \n",
    "    print(\"Train_size: \", train_size)\n",
    "    print(\"R2: \", Rscore)\n",
    "    print(\"alpha: \", alpha)\n",
    "    print(\"AIC: \", aic)\n",
    "    print(\"BIC: \", bic)\n",
    "    print(\"AICC\", aicc)\n",
    "    \n",
    "print(\"AIC values are better for the AIC model with the value of alpha\")\n",
    "print(\"AIC and BIC hold the same interpretation in terms of model comparison.\")\n",
    "print(\"That is, the larger difference in either AIC or BIC indicates stronger evidence for one model over the other \")\n",
    "print(\"(the lower the better). It's just the the AIC doesn't penalize the number of parameters as strongly as BIC.\")\n",
    "print(\" There is also a correction to the AIC (the AICc) that is used for smaller sample sizes. \")\n",
    "print(\"BIC is greater than AIC in these cases.However that need not necessarily be the case.\")\n",
    "\n",
    "#Comparing AIC and AICc\n",
    "\n",
    "print(\"When the number of observations is large the Akaike Information Criterion (AIC) and the small-sample corrected Akaike Information Criterion (AICc)\\nbecome extremely similar because AICc converges to AIC\\nTherefore we gain (or lose) almost nothing by switching between the two criteria.\")\n",
    "print(\"AIC is lower than AICc in all of the models\")\n",
    "\n",
    "random_seed = 144\n",
    "\n",
    "input_train_size = [0.8, 0.2]\n",
    "\n",
    "df = pd.read_csv('assignment4.csv')\n",
    "\n",
    "df.drop(['Unnamed: 0'], axis = 1, inplace= True)\n",
    "\n",
    "# 1 Using the dataset, create Lasso regression models to predict the ‘mpg’ for a car using 20% and 80% of the\n",
    "# training data for the 2 models\n",
    "\n",
    "#Lasso regression models predicting the mpg for both 80 and 20\n",
    "print(\"\\n######LASSO MODEL######\")\n",
    "\n",
    "for t_size in input_train_size:\n",
    "    runLassoModel(random_seed, t_size, df, 'mpg', 1)\n",
    "    print(\"\\n\")\n",
    "print(\"R2 value is 57% that is 57 %of the variation can be explained using this model\")\n",
    "\n",
    "print(\"\\n######AIC MODEL######\")\n",
    "\n",
    "# 2 and 3\n",
    "#For the above model, tune ‘alpha’ using the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) to\n",
    "# find the optimum value for it. Explain how that value for alpha was chosen. Build a model using that value of alpha.\n",
    "\n",
    "for t_size in input_train_size:\n",
    "    alpha = runCriterionModel('aic', random_seed, t_size, df, 'mpg')\n",
    "    \n",
    "    #Build a model using that value of alpha\n",
    "    print(\"\\nLasso with AIC alpha\")\n",
    "    runLassoModel(random_seed, t_size, df, 'mpg', alpha)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "print(\"\\n######BIC MODEL######\")\n",
    "\n",
    "for t_size in input_train_size:\n",
    "    alpha = runCriterionModel('bic', random_seed, t_size, df, 'mpg')\n",
    "    \n",
    "    #Build a model using that value of alpha\n",
    "    print(\"\\nLasso with BIC alpha\")\n",
    "    runLassoModel(random_seed, t_size, df, 'mpg', alpha)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"AIC values indicate a better-fit model, and a model with a delta-AIC (the difference between the two AIC\")\n",
    "print(\"values being compared) of more than -2 is considered significantly better than the model in comparison\")\n",
    "print(\"The selected alpha corresponds to the minimum of the AIC or BIC criterion\")\n",
    "print(\"It fits by computing the criterion on the specified in-sample collection. Both criteria use the training set error to\")\n",
    "print(\"predict the model generalization error and penalize this too optimistic error.\")\n",
    "print(\"This cost, however, is contingent on accurate measurement of the degrees of freedom and noise variance.\")\n",
    "print(\"Both are calculated for large samples (asymptotic results) and assume that the model is true, that is, that the data are\")\n",
    "print(\"created by the model.\")\n",
    "\n",
    "# When the problem is poorly conditioned, these models likewise likely to fail (more features than samples). \n",
    "# After that, an estimate of the noise variance is necessary.\n",
    "\n",
    "############################## 10 fold cv  ####################################\n",
    "\n",
    "#3 Build a simple regression model using 10-fold cross validation for the same data. \n",
    "#Write your observations about the R-squared values for the models and their predictions.\n",
    "\n",
    "print(\"\\n######10 fold CV######\")\n",
    "lm = LinearRegression()\n",
    "folds = KFold(n_splits = 10, shuffle = True, random_state = random_seed)\n",
    "X = df.drop(['mpg'], axis = 1).to_numpy()\n",
    "Y = df['mpg'].to_numpy()\n",
    "scores = cross_val_score(lm, X, Y, scoring='r2', cv=folds)\n",
    "#R scores\n",
    "print(\"The R2 scores are \\n\", scores)\n",
    "\n",
    "\n",
    "\n",
    "#Predictions\n",
    "y_pred = cross_val_predict(lm, X, Y, cv = folds)\n",
    "print(\"Predictions: \\n\", y_pred)\n",
    "#In k-fold cross-validation, we first shuffle our dataset so the order\n",
    "# of the inputs and outputs are completely random. We do this step to make sure that our inputs are not biased in any way.\n",
    "#from the predictions we see that cross-validation allowed us to choose a better model with a smaller order\n",
    "#avoided the overfitting problem we encountered when we don’t perform any type of cross-validation, especially with small datasets.\n",
    "\n",
    "print(\"The R2 values are better than in the rest of the models.Each of it explains the variation in the model\")\n",
    "\n",
    "print(\"alpha: one technique uses simply the training set and some information criterion to identify the optimal value of alpha, \")\n",
    "print(\"while another uses cross-validation.\")\n",
    "print(\"In this case, both ways are equally effective. Even in terms of computational performance, the in-sample hyperparameter\")\n",
    "print(\"selection seems to be effective. It can only be employed when the number of samples compared to the number of \")\n",
    "print(\"characteristics is large enough.\")\n",
    "print(\"That's why cross-validation hyperparameter optimization is a safe strategy: it works in a variety of situations.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f1f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #5 What are Randomized Control Trials: A/B Tests? \n",
    "# #What is its significance? Elaborate. Explain methods used in analyzing the RCT data?\n",
    "\n",
    "# A randomized controlled trial is a type of experiment that is used to control variables that aren't directly controlled in the experiment.\n",
    "# RCT randomly assigns subjects or volunteers to one of two groups: experimental or control, with the difference being the variable being examined.\n",
    "# Randomness in the assignment of individuals to treatments decreases biases such as selection and allocation bias, while also balancing both known and unknown prognostic factors.\n",
    "# A/B testing is a straightforward randomized controlled trial. For a single vector variable, two samples (A and B) are compared. A/B testing compares two variations of a single variable,\n",
    "# usually by comparing a subject's response to variant A to variant B and determining which form is better.\n",
    "# To make educated decisions, A/B testing and randomized controlled trials are performed.\n",
    "# They all have the same goal: to see if your hypothesis is accurate,\n",
    "# to test the effectiveness of interventions using data, and to try to identify causal links. Despite the fact that you can get answers to these concerns, the strategy you use will ultimately be determined by your primary concern and context.\n",
    "# RCTs, as opposed to simple correlations, are regarded the gold standard for learning about causal links. This is primarily due to the design's randomization, which removes (or at least reduces) bias. This type of experiment allows you to see how a\n",
    "# certain variable effects an outcome by keeping everything else constant.\n",
    "# For marketing departments and enterprises, A/B testing has become a must.A/B testing has become a go-to method\n",
    "# for marketing teams and businesses trying to make quick decisions that result in better outcomes.\n",
    "# This is due to the fact that versions can be tried simultaneously across an audience, and the data will show\n",
    "# (sometimes in real time) which performs best.\n",
    "# In RCTs, the three statistical approaches of longitudinal analysis of covariance, repeated measures analysis ,\n",
    "# and analysis of changes are most commonly employed to evaluate treatment effects\n",
    "# Analysis of RCT methods:\n",
    "# The main idea behind a summary statistic is to condense the longitudinal evolution of an outcome variable\n",
    "# across time into a single value. These summary statistics can be compared between the intervention and control\n",
    "# groups using a relatively basic cross-sectional analysis to assess the intervention's effectiveness.\n",
    "# The area under the curve (AUC) is one of the most commonly used summary statistics . However, more advanced statistical\n",
    "# methods such as mixed model analysis and generalised estimating equations are commonly used nowadays (GEE analysis)\n",
    "# Though GLM for repeated measurements is not a novel (more sophisticated) statistical technique for longitudinal data\n",
    "# analysis, it can be used to analyze a continuous outcome variable measured in an RCT with several follow-up measurements.\n",
    "# GLM for repeated measurements (also known as (multivariate) analysis of variance ((M)ANOVA) for repeated measurements) is\n",
    "# based on the same principle as the well-known paired t-test. T 1 absolute discrepancies between consecutive measurements\n",
    "# are subjected to a statistical test.\n",
    "# Multilevel analysis, hierarchical linear modeling, and random effects modeling are all terms used to describe mixed model\n",
    "# analysis. As previously stated, the main goal behind all longitudinal statistical approaches is to efficiently account\n",
    "# for'subject.' Adjusting for'subject' actually involves estimating different intercepts for all subjects in the longitudinal\n",
    "# research. The core premise of mixed model analysis in longitudinal research is that (just one) variance of those intercepts,\n",
    "# i.e. a random intercept, is estimated rather than all independent intercepts.\n",
    "# Is the sample representative of the entire population?\n",
    "# RCTs are typically conducted on a sample of people rather than the entire population.\n",
    "# It's critical for the experiment that the chosen sample accurately reflects the population's baseline characteristics.\n",
    "# \\Inferential leaps or generalizations from samples to populations aren't always easy, and they're rarely foolproof.\n",
    "# Is the sample size adequate for the target population?\n",
    "# Another crucial step is to select an appropriate sample size that yields statistically meaningful clinical differences.\n",
    "# To avoid statistical error, sample size estimation should be done just prior to the trial and should not be altered while\n",
    "# the study is running. Multiple factors influence the size of a study, including the accepted threshold of significance\n",
    "# (alpha error), study power, predicted effect size, occurrence rate in the population (prevalence rate),\n",
    "# alternative hypothesis, and population standard deviation. There are methods for calculating sample size,\n",
    "# but understanding the relationship between each element and sample size is more important.\n",
    "# Designing  Effectively\n",
    "# Experimental design is preferable to observational design because it allows for a better grasp of variables\n",
    "# \\nand the establishment of a cause–effect hypothesis. Preexperimental, quasi-experimental, and real experimental\n",
    "# study designs are all used in experiments. The absence or presence of group randomization distinguishes quasi-experimental\n",
    "# and real experimental designs.\n",
    "# Are Bias-Reducing Measures (Selection or Confouding Bias) Being Taken?\n",
    "# Interventional studies/RCTs are intended to assess the efficacy and safety of a new treatment for a\n",
    "# clinical illness. It is critical that the outcome is not random.\n",
    "# A range of methods, such as control selection, randomization, blinding, and allocation concealment, can assist\n",
    "# eliminate confounding factors and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677b87d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
